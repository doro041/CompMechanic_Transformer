{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G41BnP5b6aij",
        "outputId": "03b32bdb-0634-4307-d1d6-c02d629d1a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'epsilon-transformers'...\n",
            "remote: Enumerating objects: 5666, done.\u001b[K\n",
            "remote: Counting objects: 100% (1049/1049), done.\u001b[K\n",
            "remote: Compressing objects: 100% (506/506), done.\u001b[K\n",
            "remote: Total 5666 (delta 554), reused 889 (delta 527), pack-reused 4617\u001b[K\n",
            "Receiving objects: 100% (5666/5666), 242.68 MiB | 13.71 MiB/s, done.\n",
            "Resolving deltas: 100% (3191/3191), done.\n",
            "/content/epsilon-transformers\n",
            "Obtaining file:///content/epsilon-transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (2.0.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (0.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (3.7.1)\n",
            "Collecting wandb (from epsilon_transformers==0.1)\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (1.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (5.15.0)\n",
            "Collecting transformer-lens (from epsilon_transformers==0.1)\n",
            "  Downloading transformer_lens-2.0.0-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (7.4.4)\n",
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (1.3.1)\n",
            "Collecting black (from epsilon_transformers==0.1)\n",
            "  Downloading black-24.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy (from epsilon_transformers==0.1)\n",
            "  Downloading mypy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jaxtyping (from epsilon_transformers==0.1)\n",
            "  Downloading jaxtyping-0.2.29-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire (from epsilon_transformers==0.1)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (2.7.1)\n",
            "Collecting python-dotenv (from epsilon_transformers==0.1)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from epsilon_transformers==0.1) (4.66.4)\n",
            "Collecting boto3 (from epsilon_transformers==0.1)\n",
            "  Downloading boto3-1.34.117-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->epsilon_transformers==0.1) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->epsilon_transformers==0.1)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black->epsilon_transformers==0.1) (24.0)\n",
            "Collecting pathspec>=0.9.0 (from black->epsilon_transformers==0.1)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->epsilon_transformers==0.1) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->epsilon_transformers==0.1) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->epsilon_transformers==0.1) (4.11.0)\n",
            "Collecting botocore<1.35.0,>=1.34.117 (from boto3->epsilon_transformers==0.1)\n",
            "  Downloading botocore-1.34.117-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->epsilon_transformers==0.1)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->epsilon_transformers==0.1)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->epsilon_transformers==0.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->epsilon_transformers==0.1) (2.4.0)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping->epsilon_transformers==0.1)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->epsilon_transformers==0.1) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->epsilon_transformers==0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->epsilon_transformers==0.1) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->epsilon_transformers==0.1) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->epsilon_transformers==0.1) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->epsilon_transformers==0.1) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->epsilon_transformers==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->epsilon_transformers==0.1) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->epsilon_transformers==0.1) (2024.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->epsilon_transformers==0.1) (8.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->epsilon_transformers==0.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->epsilon_transformers==0.1) (2.18.2)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.10/dist-packages (from PyDrive->epsilon_transformers==0.1) (2.84.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyDrive->epsilon_transformers==0.1) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.10/dist-packages (from PyDrive->epsilon_transformers==0.1) (6.0.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->epsilon_transformers==0.1) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->epsilon_transformers==0.1) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->epsilon_transformers==0.1) (1.2.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->epsilon_transformers==0.1) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->epsilon_transformers==0.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->epsilon_transformers==0.1) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->epsilon_transformers==0.1) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->epsilon_transformers==0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->epsilon_transformers==0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->epsilon_transformers==0.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->epsilon_transformers==0.1) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->epsilon_transformers==0.1)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->epsilon_transformers==0.1) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->epsilon_transformers==0.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.23.0 (from transformer-lens->epsilon_transformers==0.1)\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer-lens->epsilon_transformers==0.1)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting better-abc<0.0.4,>=0.0.3 (from transformer-lens->epsilon_transformers==0.1)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Collecting datasets>=2.7.1 (from transformer-lens->epsilon_transformers==0.1)\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens->epsilon_transformers==0.1)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens->epsilon_transformers==0.1)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens->epsilon_transformers==0.1) (13.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer-lens->epsilon_transformers==0.1) (0.1.99)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer-lens->epsilon_transformers==0.1) (4.41.1)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb->epsilon_transformers==0.1)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->epsilon_transformers==0.1)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->epsilon_transformers==0.1) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->epsilon_transformers==0.1) (5.9.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->epsilon_transformers==0.1) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->epsilon_transformers==0.1)\n",
            "  Downloading sentry_sdk-2.3.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb->epsilon_transformers==0.1)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->epsilon_transformers==0.1) (67.7.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens->epsilon_transformers==0.1) (0.23.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens->epsilon_transformers==0.1) (0.4.3)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.117->boto3->epsilon_transformers==0.1) (2.0.7)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests<3,>=2.0.0 (from wandb->epsilon_transformers==0.1)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1) (3.9.5)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->epsilon_transformers==0.1)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->PyDrive->epsilon_transformers==0.1) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->PyDrive->epsilon_transformers==0.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->PyDrive->epsilon_transformers==0.1) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->PyDrive->epsilon_transformers==0.1) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.2->PyDrive->epsilon_transformers==0.1) (4.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->PyDrive->epsilon_transformers==0.1) (0.6.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->PyDrive->epsilon_transformers==0.1) (0.4.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=4.0.0->PyDrive->epsilon_transformers==0.1) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->epsilon_transformers==0.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->epsilon_transformers==0.1) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb->epsilon_transformers==0.1) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens->epsilon_transformers==0.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens->epsilon_transformers==0.1) (2.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer-lens->epsilon_transformers==0.1) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer-lens->epsilon_transformers==0.1) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->epsilon_transformers==0.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->epsilon_transformers==0.1) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens->epsilon_transformers==0.1) (4.0.3)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->epsilon_transformers==0.1)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive->epsilon_transformers==0.1) (1.63.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.2->PyDrive->epsilon_transformers==0.1) (5.3.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens->epsilon_transformers==0.1) (0.1.2)\n",
            "Building wheels for collected packages: epsilon_transformers, fire\n",
            "  Building editable for epsilon_transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for epsilon_transformers: filename=epsilon_transformers-0.1-0.editable-py3-none-any.whl size=2941 sha256=079d2b279151cd7c79d05e9c8f60eee699c9e9642fde4c63036225e1cbd47f5c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3ara_wgf/wheels/b3/d7/07/1df0a2f3e559b1b2381019428554e9038d28af7b8e5af5a83f\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=c9ae109f3a7fcb6b4040e64dfa12c8dd9e96a9a69326366699e438300f2042df\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built epsilon_transformers fire\n",
            "Installing collected packages: better-abc, xxhash, typeguard, smmap, setproctitle, sentry-sdk, requests, python-dotenv, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, jmespath, fire, fancy-einsum, einops, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, mypy, multiprocess, jaxtyping, gitdb, botocore, black, s3transfer, nvidia-cusolver-cu12, gitpython, wandb, datasets, boto3, accelerate, transformer-lens, epsilon_transformers\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.30.1 beartype-0.14.1 better-abc-0.0.3 black-24.4.2 boto3-1.34.117 botocore-1.34.117 datasets-2.19.2 dill-0.3.8 docker-pycreds-0.4.0 einops-0.8.0 epsilon_transformers-0.1 fancy-einsum-0.0.3 fire-0.6.0 gitdb-4.0.11 gitpython-3.1.43 jaxtyping-0.2.29 jmespath-1.0.1 multiprocess-0.70.16 mypy-1.10.0 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pathspec-0.12.1 python-dotenv-1.0.1 requests-2.32.3 s3transfer-0.10.1 sentry-sdk-2.3.1 setproctitle-1.3.3 smmap-5.0.1 transformer-lens-2.0.0 typeguard-2.13.3 wandb-0.17.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Clone the specific branch hackathon-prep\n",
        "!git clone --branch hackathon-prep https://github.com/adamimos/epsilon-transformers.git\n",
        "%cd epsilon-transformers\n",
        "\n",
        "# Step 2: Install the necessary dependencies\n",
        "!pip install -e .\n",
        "\n",
        "# Step 3: Install gdown if not already installed\n",
        "#!pip install gdown\n",
        "\n",
        "# Step 4: Download the RRXOR experiment data\n",
        "#!gdown \"https://drive.google.com/uc?id=1PYMcdvvJ_FW31rQDBmnNKz9LOyFEcfqQ\" -O vfs4q106-rrxor.zip\n",
        "\n",
        "# Step 5: Unzip the data in the correct location\n",
        "#!unzip vfs4q106-rrxor.zip -d examples/models/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from epsilon_transformers.process.processes import ZeroOneR, GoldenMean, Mess3\n"
      ],
      "metadata": {
        "id": "L5De9TYs6fV0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from epsilon_transformers.process.Process import Process\n",
        "import numpy as np\n",
        "\n",
        "class GluedProcess(Process):\n",
        "  def __init__(self, proc1, proc2, join_on=dict(), weights=(0.5,0.5)):\n",
        "        # join_on is a dictionary that maps vocubulary from Process 2 to Process 1\n",
        "        # So join_on = {0: 1} means that emitting a \"0\" in Process 2 looks the same\n",
        "        # as emitting a \"1\" in Process 1, but all other vocubulary of Process 1\n",
        "        # is discernable.\n",
        "\n",
        "        self.name = proc1.name + \"+\" + proc2.name\n",
        "        self.proc1 = proc1\n",
        "        self.proc2 = proc2\n",
        "        self.weights = weights\n",
        "        self.join_on = join_on\n",
        "        super().__init__()\n",
        "\n",
        "  def _create_hmm(self):\n",
        "        n_states = len(self.proc1.state_names_dict)\n",
        "        state_names = self.proc1.state_names_dict.copy()\n",
        "        for key, val in self.proc2.state_names_dict.items():\n",
        "          # choose a unique name for merged state in case it is already occupied\n",
        "          while key in state_names:\n",
        "            key += \"_\"\n",
        "          state_names[key] = n_states\n",
        "          n_states += 1\n",
        "\n",
        "        # For a combination in which the vocabulary is disjoint, the vocab\n",
        "        # size is the sum, else the larger of the two\n",
        "        vocab_len = self.proc1.vocab_len + self.proc2.vocab_len - len(self.join_on)\n",
        "        T = np.zeros((vocab_len, n_states, n_states))\n",
        "\n",
        "        # Copying over values from Proc1\n",
        "        shape1 = self.proc1.transition_matrix.shape\n",
        "        print(shape1)\n",
        "        T[:shape1[0],:shape1[1],:shape1[2]] = self.proc1.transition_matrix\n",
        "\n",
        "        # Copying from Proc2\n",
        "        new_v = 0 # This counts the number of new vocabulary tokens\n",
        "        for v in range(self.proc2.vocab_len):\n",
        "          if v in self.join_on:\n",
        "            T[self.join_on[v],shape1[1]:, shape1[2]:] = self.proc2.transition_matrix[v,:,:]\n",
        "          else:\n",
        "            T[shape1[0]+new_v,shape1[1]:, shape1[2]:] = self.proc2.transition_matrix[v,:,:]\n",
        "            new_v += 1\n",
        "\n",
        "        print(T)\n",
        "        return T, state_names\n",
        "\n",
        "  @property\n",
        "  def steady_state_vector(self):\n",
        "      steady_state_vector = np.concatenate((self.proc1.steady_state_vector * self.weights[0], self.proc2.steady_state_vector * self.weights[1]))\n",
        "      #steady_state_vector = np.ones((self.num_states))\n",
        "\n",
        "      out = steady_state_vector / steady_state_vector.sum()\n",
        "      assert out.ndim == 1\n",
        "      assert len(out) == self.num_states\n",
        "      return out\n",
        "\n",
        "\n",
        "class BiasedCoin(Process):\n",
        "    def __init__(self, bias: float = 0.5):\n",
        "        self.name = \"bc\"\n",
        "        self.p = bias\n",
        "        super().__init__()\n",
        "\n",
        "    def _create_hmm(self):\n",
        "        T = np.zeros((2, 1, 1))\n",
        "        state_names = {\"0\": 0}\n",
        "        T[0, state_names[\"0\"], state_names[\"0\"]] = self.p\n",
        "        T[1, state_names[\"0\"], state_names[\"0\"]] = 1-self.p\n",
        "\n",
        "        return T, state_names\n",
        "\n",
        "class D3(Process):\n",
        "    def __init__(self,a,b):\n",
        "        self.name = \"d3\"\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        super().__init__()\n",
        "\n",
        "    def _create_hmm(self):\n",
        "        T = np.zeros((3, 1, 1))\n",
        "        state_names = {\"0\": 0}\n",
        "        T[0, state_names[\"0\"], state_names[\"0\"]] = self.a\n",
        "        T[1, state_names[\"0\"], state_names[\"0\"]] = self.b\n",
        "        T[2, state_names[\"0\"], state_names[\"0\"]] = 1-self.a-self.b\n",
        "\n",
        "        return T, state_names"
      ],
      "metadata": {
        "id": "CIaVSyo3puiV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from epsilon_transformers.visualization.graph import transition_matrix_to_graph, visualize_graph\n",
        "\n",
        "graph = transition_matrix_to_graph(transition_matrix=gp.transition_matrix,state_names=gp.state_names_dict)\n",
        "visualize_graph(graph, draw_mixed_state=True, layout=\"circular\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "jpz55-CBtYO2",
        "outputId": "14100811-93b2-47e7-bcb3-c8ba988400b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gp' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-766afeab2735>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mepsilon_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransition_matrix_to_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransition_matrix_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_names_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvisualize_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_mixed_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"circular\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from epsilon_transformers.visualization.plots import _project_to_simplex\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "proc1 = D3(0.6,0.2)\n",
        "proc2 = D3(0.2,0.6)\n",
        "proc3 = D3(0.2,0.2)\n",
        "process = GluedProcess(proc3, GluedProcess(proc1, proc2, {0:0,1:1,2:2}), {0:0,1:1,2:2}, weights=(1, 2))\n",
        "mixed_state_tree = process.derive_mixed_state_presentation(depth=5)\n",
        "MSP_transition_matrix = mixed_state_tree.build_msp_transition_matrix()\n",
        "\n",
        "tree_paths, tree_beliefs = mixed_state_tree.paths_and_belief_states\n",
        "\n",
        "msp_beliefs = [tuple(round(b, 5) for b in belief) for belief in tree_beliefs]\n",
        "msp_belief_index = {b: i for i, b in enumerate(set(msp_beliefs))}\n",
        "ground_truth_simplex = _project_to_simplex(np.array(list(msp_belief_index.keys())))\n",
        "plt.figure(figsize=(4.5, 4))\n",
        "plt.scatter(ground_truth_simplex[0], ground_truth_simplex[1], c=[k for k in list(msp_belief_index.keys())], alpha=.75, s=5)\n",
        "plt.title(\"Ground Truth Simplex\")\n",
        "plt.gca().set_axis_off()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RoJ-QZgithtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer training\n"
      ],
      "metadata": {
        "id": "0j8974VJ6bwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "eBoj5FRhEB4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from epsilon_transformers.persistence import S3Persister, HackyPersister\n",
        "from epsilon_transformers.training.configs.model_configs import RawModelConfig\n",
        "from epsilon_transformers.process.processes import RRXOR, Mess3\n",
        "from epsilon_transformers.analysis.activation_analysis import get_beliefs_for_transformer_inputs\n",
        "from epsilon_transformers.visualization.plots import _project_to_simplex\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "Qj86qldcENSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transformer_data_from_process(process, n_ctx):\n",
        "    mixed_state_tree = process.derive_mixed_state_presentation(depth=n_ctx+1)\n",
        "    tree_paths, tree_beliefs = mixed_state_tree.paths_and_belief_states\n",
        "    transformer_data = [x for x in tree_paths if len(x) == n_ctx+1]\n",
        "    transformer_data = torch.tensor(transformer_data)\n",
        "    transformer_input = transformer_data[:, :-1]\n",
        "    transformer_target = transformer_data[:, 1:]\n",
        "    return transformer_input, transformer_target\n",
        "\n",
        "def get_lower_bound_for_cross_entropy(process, n_ctx):\n",
        "    mixed_state_tree = process.derive_mixed_state_presentation(depth=n_ctx+1)\n",
        "    myopic_entropy = mixed_state_tree.myopic_entropy\n",
        "    return myopic_entropy[1:]"
      ],
      "metadata": {
        "id": "z83TDfiCEVX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = 2,\n",
        "    n_heads = 3,\n",
        "    d_model = 9,\n",
        "    d_head = 3,\n",
        "    d_mlp = 9,\n",
        "    act_fn = \"relu\",\n",
        "    normalization_type=None,\n",
        "    d_vocab=3,\n",
        "    n_ctx=6,\n",
        "    init_weights=True,\n",
        "    device=device,\n",
        "    seed = 999,\n",
        ")"
      ],
      "metadata": {
        "id": "Skv_LKl8Ekw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from typing import List, Tuple, Iterable\n",
        "\n",
        "\n",
        "class ProcessDataset(IterableDataset):\n",
        "\n",
        "    def __init__(self, process, sequence_length, num_samples, fixed=False):\n",
        "        super().__init__()\n",
        "        self.process = process\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_samples = num_samples\n",
        "        self.fixed = fixed\n",
        "        if self.fixed:\n",
        "          self.samples = list(self._get_samples())\n",
        "        else:\n",
        "          self.samples = None\n",
        "\n",
        "    def _get_samples(self):\n",
        "      return process.yield_emissions(\n",
        "            sequence_len=self.num_samples * (self.sequence_length + 1)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __iter__(self) -> Iterable[Tuple[List[int]]]:\n",
        "        samples = self._get_samples() if self.samples is None else iter(self.samples)\n",
        "        for _ in range(self.num_samples):\n",
        "            process_history = [\n",
        "                next(samples) for _ in range(self.sequence_length + 1)\n",
        "            ]\n",
        "            yield (process_history[:-1], process_history[1:])\n",
        "\n",
        "\n",
        "def process_dataset_collate_fn(batch: List[Tuple[List[int]]]):\n",
        "    data = [x[0] for x in batch]\n",
        "    labels = [x[1] for x in batch]\n",
        "    return torch.tensor(data, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "train_dataset = ProcessDataset(process, sequence_length=cfg.n_ctx, num_samples=10000,\n",
        "                                     fixed=False)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=100, collate_fn=process_dataset_collate_fn)\n",
        "val_dataset = ProcessDataset(process, sequence_length=cfg.n_ctx, num_samples=1000,\n",
        "                                   fixed=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=100, collate_fn=process_dataset_collate_fn)"
      ],
      "metadata": {
        "id": "xuMTautumzgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proc1 = D3(0.6,0.2)\n",
        "proc2 = D3(0.2,0.6)\n",
        "proc3 = D3(0.2,0.2)\n",
        "process = GluedProcess(proc3, GluedProcess(proc1, proc2, {0:0,1:1,2:2}), {0:0,1:1,2:2}, weights=(1, 2))\n",
        "transformer_inputs, transformer_targets = get_transformer_data_from_process(process, cfg.n_ctx)\n",
        "minimum_loss = np.mean(get_lower_bound_for_cross_entropy(process, cfg.n_ctx))\n",
        "print(f\"Minimum Loss: {minimum_loss}\")\n",
        "transformer_inputs = transformer_inputs.to(device)\n",
        "transformer_targets = transformer_targets.to(device)\n",
        "\n",
        "mixed_state_tree = process.derive_mixed_state_presentation(depth=cfg.n_ctx)\n",
        "MSP_transition_matrix = mixed_state_tree.build_msp_transition_matrix()\n",
        "\n",
        "tree_paths, tree_beliefs = mixed_state_tree.paths_and_belief_states\n",
        "msp_beliefs = [tuple(round(b, 5) for b in belief) for belief in tree_beliefs]\n",
        "msp_belief_index = {b: i for i, b in enumerate(set(msp_beliefs))}"
      ],
      "metadata": {
        "id": "OYnAUPesEstY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer(cfg)\n"
      ],
      "metadata": {
        "id": "7xKW7GVvFMaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "for epoch in tqdm(range(10000)):\n",
        "    train_logits = model(transformer_inputs)\n",
        "    train_loss = loss_fn(train_logits.view(-1, cfg.d_vocab), transformer_targets.flatten())\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{10000}, Loss: {train_loss.item()/minimum_loss*100} percent of minimum, LR: {optimizer.param_groups[0]['lr']}\")\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "id": "iylTEAD_FQvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# 保存整个模型到当前工作目录\n",
        "filename = 'd3.pth'\n",
        "torch.save(model, filename)\n",
        "\n"
      ],
      "metadata": {
        "id": "1TrDvgmKCXq4"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr2SMe39Uo4D",
        "outputId": "15360f8d-22e0-4bb5-a78c-7818c897ccd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 52\n",
            "drwxr-xr-x 8 root root  4096 Jun  2 19:51 epsilon-transformers\n",
            "drwxr-xr-x 8 root root  4096 Jun  2 17:28 epsilon_transformers\n",
            "drwxr-xr-x 2 root root  4096 Jun  2 19:55 epsilon_transformers.egg-info\n",
            "drwxr-xr-x 2 root root  4096 Jun  2 17:26 examples\n",
            "-rw-r--r-- 1 root root 19574 Jun  2 20:01 my_model.pth\n",
            "-rw-r--r-- 1 root root   584 Jun  2 17:26 pyproject.toml\n",
            "-rw-r--r-- 1 root root  2686 Jun  2 17:26 README.md\n",
            "drwxr-xr-x 2 root root  4096 Jun  2 17:26 scripts\n",
            "drwxr-xr-x 2 root root  4096 Jun  2 17:26 tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载模型\n",
        "loaded_model = torch.load(filename)\n",
        "\n",
        "# 检查模型是否相同\n",
        "# 这可以通过比较某些输出或模型参数来进行\n",
        "pruned_paths = tree_paths\n",
        "input_example = torch.randint(low=0, high=proc1.vocab_len, size=(1, cfg.n_ctx))  # 假设词汇表大小为vocab_size\n",
        "input_example = input_example.long()  # 转换输入数据类型\n",
        "original_output = model(input_example)\n",
        "loaded_output = loaded_model(input_example)\n",
        "print(torch.equal(original_output, loaded_output))  # 输出应该是 True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQYwhbJDVBbB",
        "outputId": "5257a7dd-07df-497c-dcc5-3a94b48e0f95"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "transformer_inputs = [x for x in pruned_paths if len(x) == cfg.n_ctx]\n",
        "transformer_inputs = torch.tensor(transformer_inputs, dtype=torch.int).to(device)\n",
        "\n",
        "# print first few batches\n",
        "print(transformer_inputs)\n",
        "transformer_input_beliefs, transformer_input_belief_indices = get_beliefs_for_transformer_inputs(transformer_inputs, msp_belief_index, pruned_paths, tree_beliefs)\n",
        "print(f\"Transformer Input Beliefs: {transformer_input_beliefs.shape}, Transformer Input Belief Indices: {transformer_input_belief_indices.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufjqUp0sWKNk",
        "outputId": "b0b03c67-a424-4b64-eb26-f8223efa8423"
      },
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 2, 1, 2, 1, 1],\n",
            "        [1, 2, 0, 1, 1, 1],\n",
            "        [1, 2, 1, 1, 1, 0],\n",
            "        ...,\n",
            "        [0, 1, 0, 1, 2, 0],\n",
            "        [2, 0, 2, 0, 0, 2],\n",
            "        [2, 2, 1, 0, 0, 0]], dtype=torch.int32)\n",
            "Transformer Input Beliefs: torch.Size([729, 6, 3]), Transformer Input Belief Indices: torch.Size([729, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = torch.load(filename)\n",
        "_, activations = model.run_with_cache(transformer_inputs, names_filter=lambda x: 'resid_post' in x)\n",
        "print(activations.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW9WHw3CY2Wl",
        "outputId": "eb628991-c436-4ddc-f2db-222be4a9669d"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['blocks.0.hook_resid_post', 'blocks.1.hook_resid_post'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we now have activations [batch, n_ctx, d_model]\n",
        "# and we have transformer_input_beliefs [batch, n_ctx, belief_dim]\n",
        "# and we have transformer_input_belief_indices [batch, n_ctx]\n",
        "\n",
        "# in the end we want to do linear regression between the activations and the transformer_input_beliefs\n",
        "def run_activation_to_beliefs_regression(activations, ground_truth_beliefs):\n",
        "\n",
        "    # make sure the first two dimensions are the same\n",
        "    assert activations.shape[0] == ground_truth_beliefs.shape[0]\n",
        "    assert activations.shape[1] == ground_truth_beliefs.shape[1]\n",
        "\n",
        "    # flatten the activations\n",
        "    batch_size, n_ctx, d_model = activations.shape\n",
        "    belief_dim = ground_truth_beliefs.shape[-1]\n",
        "    activations_flattened = activations.view(-1, d_model) # [batch * n_ctx, d_model]\n",
        "    ground_truth_beliefs_flattened = ground_truth_beliefs.view(-1, belief_dim) # [batch * n_ctx, belief_dim]\n",
        "\n",
        "    # run the regression\n",
        "    regression = LinearRegression()\n",
        "    regression.fit(activations_flattened, ground_truth_beliefs_flattened)\n",
        "\n",
        "    # get the belief predictions\n",
        "    belief_predictions = regression.predict(activations_flattened) # [batch * n_ctx, belief_dim]\n",
        "    belief_predictions = belief_predictions.reshape(batch_size, n_ctx, belief_dim)\n",
        "\n",
        "    return regression, belief_predictions\n"
      ],
      "metadata": {
        "id": "-dZE9wz0bPL2"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acts = torch.cat([v for k, v in activations.items()], dim=-1)\n",
        "regression, belief_predictions = run_activation_to_beliefs_regression(acts, transformer_input_beliefs)\n",
        "print(f\"Shape of belief_predictions: {belief_predictions.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6Hj5Rc_bSbg",
        "outputId": "0910bb78-7303-489f-f999-acbe49e1be70"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of belief_predictions: (729, 6, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "AcIPPJhvjVIK",
        "outputId": "47895459-47bb-4c5e-963d-bcc6ee529940"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'regression' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f23253d59639>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'regression' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from epsilon_transformers.analysis.activation_analysis import find_msp_subspace_in_residual_stream\n",
        "from epsilon_transformers.process.processes import Mess3\n",
        "\n",
        "\n",
        "belief_predictions_flattened = belief_predictions.reshape(-1, 3)\n",
        "transformer_input_belief_flattened = transformer_input_beliefs.reshape(-1, 3)\n",
        "\n",
        "# project to simplex\n",
        "belief_true_projected = _project_to_simplex(transformer_input_belief_flattened)\n",
        "belief_pred_projected = _project_to_simplex(belief_predictions_flattened)\n",
        "\n",
        "rgb_colors =  transformer_input_belief_flattened.cpu().numpy()\n",
        "#rgb_colors = rgb_colors.astype(int)\n",
        "\n",
        "sns.set_context(\"paper\")\n",
        "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
        "\n",
        "# Plotting the true beliefs projected onto the simplex\n",
        "axes[0].scatter(belief_true_projected[0], belief_true_projected[1], marker='.', c=rgb_colors, alpha=0.2, s=0.5)\n",
        "axes[0].axis('off')\n",
        "axes[0].set_title(\"Ground Truth Simplex\")\n",
        "\n",
        "# Plotting the predicted beliefs projected onto the simplex\n",
        "axes[1].scatter(belief_pred_projected[0], belief_pred_projected[1], marker='.', c=rgb_colors, alpha=0.3, s=0.01)\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title(\"Residual Stream Simplex\")\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the figure\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "QsNXkP8XdLs7",
        "outputId": "9262d014-95aa-442d-ac4b-4ba6d93caab5"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zya8Ym2ngaJR"
      }
    }
  ]
}